{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#モジュールのインポート\n",
    "import torch\n",
    "from dalle_pytorch import DiscreteVAE\n",
    "\n",
    "#dVAEのパラメータ定義\n",
    "vae = DiscreteVAE(\n",
    "    image_size = 256,\n",
    "    num_layers = 3,           # ダウンサンプリングの数。ex. 256 / (2 ** 3) = (32 x 32 feature map)\n",
    "    num_tokens = 8192,        # visual tokensの数。論文では8192を使用したが、もっと小さくすることができる\n",
    "    codebook_dim = 512,       # codebookの次元\n",
    "    hidden_dim = 64,          # hiddenの次元\n",
    "    num_resnet_blocks = 1,    # resnetのブロックの数\n",
    "    temperature = 0.9,        # gumbel softmax温度。これが低いほど、離散化は難しくなる\n",
    "    straight_through = False, # gumbel softmaxのためのstraight-through。どちらが良いかわからない\n",
    ")\n",
    "\n",
    "images = torch.randn(4, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b89f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#モジュールのインポート\n",
    "import torch\n",
    "from dalle_pytorch import DiscreteVAE, DALLE\n",
    "\n",
    "#dVAEのパラメータ定義\n",
    "vae = DiscreteVAE(\n",
    "    image_size = 256,\n",
    "    num_layers = 3,\n",
    "    num_tokens = 8192,\n",
    "    codebook_dim = 1024,\n",
    "    hidden_dim = 64,\n",
    "    num_resnet_blocks = 1,\n",
    "    temperature = 0.9\n",
    ")\n",
    "\n",
    "#DALLEのパラメータ定義\n",
    "dalle = DALLE(\n",
    "    dim = 1024,\n",
    "    vae = vae,                  # （1）画像シーケンスの長さと（2）画像トークンの数を自動的に推測\n",
    "    num_text_tokens = 10000,    # テキストの語彙サイズ\n",
    "    text_seq_len = 256,         # テキストシーケンスの長さ\n",
    "    depth = 12,                 # 64を目指すべき\n",
    "    heads = 16,                 # attention headの数\n",
    "    dim_head = 64,              # attention headの次元\n",
    "    attn_dropout = 0.1,         # attention dropout\n",
    "    ff_dropout = 0.1            # feedforward dropout\n",
    ")\n",
    "\n",
    "text = torch.randint(0, 10000, (4, 256))\n",
    "images = torch.randn(4, 3, 256, 256)\n",
    "mask = torch.ones_like(text).bool()\n",
    "\n",
    "loss = dalle(text, images, mask = mask, return_loss = True)\n",
    "loss.backward()\n",
    "\n",
    "# 大量のデータを使用して上記を長時間実行\n",
    "\n",
    "images = dalle.generate_images(text, mask = mask)\n",
    "images.shape # (4, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741db984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#上記2つを実行した場合は実行する必要なし\n",
    "#時間短縮用の事前学習データ読み込み\n",
    "import torch\n",
    "from dalle_pytorch import OpenAIDiscreteVAE, DALLE\n",
    "\n",
    "vae = OpenAIDiscreteVAE()       # 事前学習済みのOpenAIVAEをロード\n",
    "\n",
    "dalle = DALLE(\n",
    "    dim = 1024,\n",
    "    vae = vae,                  # （1）画像シーケンスの長さと（2）画像トークンの数を自動的に推測\n",
    "    num_text_tokens = 10000,    # テキストの語彙サイズ\n",
    "    text_seq_len = 256,         # テキストシーケンスの長さ\n",
    "    depth = 1,                  # 64を目指すべき\n",
    "    heads = 16,                 # attention headの数\n",
    "    dim_head = 64,              # attention headの次元\n",
    "    attn_dropout = 0.1,         # attention dropout\n",
    "    ff_dropout = 0.1            # feedforward dropout\n",
    ")\n",
    "\n",
    "text = torch.randint(0, 10000, (4, 256))\n",
    "images = torch.randn(4, 3, 256, 256)\n",
    "mask = torch.ones_like(text).bool()\n",
    "\n",
    "loss = dalle(text, images, mask = mask, return_loss = True)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e498f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#モジュールのインポート\n",
    "import torch\n",
    "from dalle_pytorch import CLIP\n",
    "\n",
    "clip = CLIP(\n",
    "    dim_text = 512,\n",
    "    dim_image = 512,\n",
    "    dim_latent = 512,\n",
    "    num_text_tokens = 10000,\n",
    "    text_enc_depth = 6,\n",
    "    text_seq_len = 256,\n",
    "    text_heads = 8,\n",
    "    num_visual_tokens = 512,\n",
    "    visual_enc_depth = 6,\n",
    "    visual_image_size = 256,\n",
    "    visual_patch_size = 32,\n",
    "    visual_heads = 8\n",
    ")\n",
    "\n",
    "text = torch.randint(0, 10000, (4, 256))\n",
    "images = torch.randn(4, 3, 256, 256)\n",
    "mask = torch.ones_like(text).bool()\n",
    "\n",
    "#損失関数\n",
    "loss = clip(text, images, text_mask = mask, return_loss = True)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, scores = dalle.generate_images(text, mask = mask, clip = clip)\n",
    "\n",
    "scores.shape # (2,)\n",
    "images.shape # (2, 3, 256, 256)\n",
    "\n",
    "# 論文では512 samplingのtop 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995876ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DALLEのパラメータ定義①\n",
    "dalle = DALLE(\n",
    "    dim = 1024,\n",
    "    vae = vae,\n",
    "    num_text_tokens = 10000,\n",
    "    text_seq_len = 256,\n",
    "    depth = 64,\n",
    "    heads = 16,\n",
    "    reversible = True  # <-- reversible networks https://arxiv.org/abs/2001.04451\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0017fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DALLEのパラメータ定義②\n",
    "dalle = DALLE(\n",
    "    dim = 1024,\n",
    "    vae = vae,\n",
    "    num_text_tokens = 10000,\n",
    "    text_seq_len = 256,\n",
    "    depth = 64,\n",
    "    heads = 16,\n",
    "    reversible = True,\n",
    "    attn_types = ('full', 'axial_row', 'axial_col', 'conv_like')  # cycles between these four types of attention\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b0499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DALLEのパラメータ定義③\n",
    "dalle = DALLE(\n",
    "    dim = 512,\n",
    "    vae = vae,\n",
    "    num_text_tokens = 10000,\n",
    "    text_seq_len = 256,\n",
    "    depth = 64,\n",
    "    heads = 8,\n",
    "    attn_types = ('full', 'sparse')  # interleave sparse and dense attention for 64 layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d9ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
